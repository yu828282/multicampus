# 머신러닝 종류
1. 지도학습 (Supervised Learning) - 정답을 제시하고 데이터를 기계에 학습
     1. 회귀 (Regression) : 데이터를 입력하면 출력으로 숫자를 반환 하는 방법. 예측결과가 숫자.
        1. 선형 회귀 : 데이터에 가장 많이 적용되는 직선을 구하는 분석 방법. 가장 간단한 모델. 예) 매출 예측, 리스크 평가, 스포츠 분석..
        2. 로지스틱 회귀 : 분류 문제를 해결하는 모델, 여러 변수에서 특정 이벤트가 발생할 확률을 예측, 사건의 발생률을 예측하는 것이 좋기 때문에, 로지스틱 회귀는 미지의 병을 발견하거나, 기상 관측 데이터로부터의 토사 재해 발생 예측 등에 활용

     2. 분류 (Classification) : 데이터를 입력하면 출력으로 데이터의 속성이나 종류를 반환 하는 방법. 예측결과가 숫자가 아님. 데이터를 분류하는 구분선 또는 구분면을 찾아서 분류
        1. SVM ( support vector machine, 지지도 벡터 기계 ) - 데이터 점을 새로운 고차원 공간에 배치
            - 핵 함수(kernel function)을 통해서 데이터 점들간의 간극이 가장 큰(여유도가 가장 큰) 초 평면을 탐색
            - 해당 초평면을 기준으로 데이터 점들을 분류 
            - SVM 장 단점 장점 : 간단한 분류 문제를 푸는데 적합 단점 : 대규모 데이터 처리 불가. 이미지 처리 분류 부적합 

        2. 결정 트리 (decision tree)출현 -> 모이면 랜덤 포레스트
           - 나무 모양으로 순서에 따라 결정을 반복
           - 2010년 대부터 커널 메소드 보다 더 인기를 끌었다. (커널 메소드 : 저차원에서 선형으로 분류가 불가능한 데이터를 고차원으로 보내서 고차원에서 선형으로 분류하는 방법)
           - 2014년도 대세가 된다.    
        3. 그래디언트 부스팅 머신
         - 여러 결정트리를 순서대로 배치해 경사도를 증폭
         - 랜덤 포레스트가 병렬처리라면 이방법은 순차 처리이다.
         - XGBoot를 경진 대회에서 자주 사용하게 됬다.    ​ 
        4. 나이브 베이즈
         - 분류 문제를 풀기 위한 모델로, 확률론의 정리인 베이즈의 정리알고리즘 적용
         - 계산량이 적고 처리가 빠르기 때문에 대규모 데이터에도 사용
         - 텍스트 분류, 메일 스팸, 비 스팸 판정등 사용


2. 비지도학습 - 정답을 줄수 없는 방대한 데이터로 부터 자동적으로 산출한 특징량으로 부터 구조나, 경향, 법칙 등을 이용한 기계에 학습 시키는 방법.  y항이 없다
   1. k-means clustering : 데이터를 성질이 가까운 분류끼리 그룹으로 나누는 방법, 클러스터링의 가장 쉬운 기술로 빅 데이터를 분석할 때 자주 사용되는 가장 중요한 기술
   2. k-nearest neighbor(KNN) : 패턴 인식에 자주 사용되며(딥러닝), 특징 공간에서 가장 가까운 훈련 예에 기초한 분류 방법, 지연 학습으로 분류

3. 강화학습 - 스스로 시행 착오을 일으켜서 최적의 행동을 찾는 학습으로 최근의 목표를 달성하고 보상을 주는 것으로 향상하는 방법


# 머신러닝 목적 : 주어진 학습 데이터에 대해 손실 함수가 최소가 되는 가중치와 바이어스를 찾는 것
학습 (러닝) : 계산값 Y와 정답 T와의 차이를 나타내는 손실값(비용함수)loss가 최소가 될 때까지 가중치 W, 바이어스 b를 최적화 시키는 과정
학습데이터 -> [입력층, 출력층 (X*w+b)] -> Y -> 손실함수 = ML -> linear regression
학습데이터 -> [입력층, 출력층 (X*w+b)] -> act_fun -> 손실함수 = ML -> 로지스틱 리그레션
학습데이터 -> [입력층, 은닉층, 출력층 (X*w+b)] -> Y -> 손실함수 = DL

​

# 모형생성 순서 :  데이터 수집 -> 데이터 분할 -> 모델 선택(분류기별 정확도 점수)
비용함수 : 학습데이터에 존재하는 전체의 에러 정도
MSE (Mean Squared Error) = 오차 제곱 평균 = 연속형변수값을 예측할 때 사용 = 다중분류시 사용 -> root로 RMSE
MAE (Mean Absolute Error, 평균 절대 오차) 
결정계수(R2) : Y의 총 변동량 중에서 X에 의해서 설명된 량으로 '표본에 대한 회귀모형의 설명'. 값이 클수록(1에 가까울수록) 모형이 적합

회귀 모형 평가 시는 r2_score(결정계수(R의 제곱)를 알 수 있게 해 주는 함수) 사용
분류 모형 평가 시에는 accuracy_score(정확도를 알 수 있게 해 주는 함수)를 사용

# 모델 선택방법(사이킷런) : 
추정기 성능 평가를 위한 교차 검증 -> 한 가지 추정기의 하이퍼 파라미터 조율 -> 예측 품질의 정량화를 위한 계량과 점수 매기기 -> 평가한 모형의 점수를 검증 곡선으로 그려 보기

# 모델 성능평가
1. 분류기별 정확도 점수
all_estimators(type_filter="regressor")
all_estimators(type_filter="classifier")
all_estimators(type_filter="transformer")  변환기

2. k겹 교차 검증(k-fold cross validation)​

3. 사이킷런 치트 시트 이용 . 인스턴스화 -> fit -> Predict / transform (**3단계 필요**)​

# 객체 단위 : Estimator(객체 인스턴스), fit, Predict, transform

1. Estimator : 학습데이터를 기반으로 모델을 적합시키고 새로운 데이터와 어떤 특성을 추론할 수 있는 객체. 모든 알고리즘의 최상위 클래스.
   
2. fit : Estimator의 내부 메소드. 모델의 성능을 검증하는 함수(하이퍼파라미터)를 Estimator의 매개인자로 받아 실행.
지도학습 알고리즘은 학습데이터와 레벨데이터를 매개인자로, 비지도학습알고리즘은 학습데이터만 매개인자로 전달한다. 
   ex) fit(X, y, sample_weight=None) #X -> 대문자면 독립변수 여러개

3. Predict : 인스턴스화 하고 fit으로 학습을 마친 모델을 사용. 입력데이터에 대한 모델의 예측 결과를 리턴한다.

4. transform : 피처 처리 기능을 transform메소드로 실행하고 결과를 리턴 [(전처리기능)] -> 원핫인코딩, 정규화, 스케일링 등

# API 선언 방법 순서 : 
1. 인스턴스화 -> fit -> Predict / transform (3단계 필요)
2. 분류 알고리즘(classifier), 회귀 알고리즘(regressor) 확인
3. 이후 전처리 작업 - 스케일링 적용